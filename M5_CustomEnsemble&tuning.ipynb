{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import math\n",
    "import pickle\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the data from files\n",
    "calendar_df = pd.read_csv('calendar.csv')\n",
    "sales_eval_df = pd.read_csv('sales_train_evaluation.csv')\n",
    "prices_df = pd.read_csv('sell_prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of calendar_df is  (1969, 14)\n",
      "top 2 rows of calendar_df\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>weekday</th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>d</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>11101</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  wm_yr_wk   weekday  wday  month  year    d event_name_1  \\\n",
       "0  2011-01-29     11101  Saturday     1      1  2011  d_1          NaN   \n",
       "1  2011-01-30     11101    Sunday     2      1  2011  d_2          NaN   \n",
       "\n",
       "  event_type_1 event_name_2 event_type_2  snap_CA  snap_TX  snap_WI  \n",
       "0          NaN          NaN          NaN        0        0        0  \n",
       "1          NaN          NaN          NaN        0        0        0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('shape of calendar_df is ',calendar_df.shape)\n",
    "print('top 2 rows of calendar_df')\n",
    "calendar_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of sales_eval_df is  (30490, 1947)\n",
      "top 2 rows of sales_eval_df\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1932</th>\n",
       "      <th>d_1933</th>\n",
       "      <th>d_1934</th>\n",
       "      <th>d_1935</th>\n",
       "      <th>d_1936</th>\n",
       "      <th>d_1937</th>\n",
       "      <th>d_1938</th>\n",
       "      <th>d_1939</th>\n",
       "      <th>d_1940</th>\n",
       "      <th>d_1941</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1947 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  d_1  d_2  d_3  d_4  ...  d_1932  d_1933  d_1934  d_1935  d_1936  \\\n",
       "0       CA    0    0    0    0  ...       2       4       0       0       0   \n",
       "1       CA    0    0    0    0  ...       0       1       2       1       1   \n",
       "\n",
       "   d_1937  d_1938  d_1939  d_1940  d_1941  \n",
       "0       0       3       3       0       1  \n",
       "1       0       0       0       0       0  \n",
       "\n",
       "[2 rows x 1947 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('shape of sales_eval_df is ',sales_eval_df.shape)\n",
    "print('top 2 rows of sales_eval_df')\n",
    "sales_eval_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of prices_df is  (6841121, 4)\n",
      "top 2 rows of prices_df\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>sell_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11325</td>\n",
       "      <td>9.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11326</td>\n",
       "      <td>9.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  store_id        item_id  wm_yr_wk  sell_price\n",
       "0     CA_1  HOBBIES_1_001     11325        9.58\n",
       "1     CA_1  HOBBIES_1_001     11326        9.58"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('shape of prices_df is ',prices_df.shape)\n",
    "print('top 2 rows of prices_df')\n",
    "prices_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Feature Engineering </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference: https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                       df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  0.12 Mb (41.9% reduction)\n",
      "Mem. usage decreased to 130.48 Mb (37.5% reduction)\n",
      "Mem. usage decreased to 102.64 Mb (77.4% reduction)\n"
     ]
    }
   ],
   "source": [
    "#here we are inserting the columns for test set from days d_1942 to d_1969 which we need to forecast sales as nan \n",
    "for i in range(1942,1970):\n",
    "    sales_eval_df['d_'+str(i)] = np.nan\n",
    "    sales_eval_df['d_'+str(i)] = sales_eval_df['d_'+str(i)].astype(np.float16)\n",
    "\n",
    "#to reduce the memory usage by changing the dtypes of columns of the dataframes\n",
    "calendar_df = reduce_mem_usage(calendar_df)\n",
    "prices_df = reduce_mem_usage(prices_df)\n",
    "sales_eval_df = reduce_mem_usage(sales_eval_df)\n",
    "\n",
    "#to transform the dataframe into vertical rows as each corresponds to each day sales of an item from a particular store\n",
    "sales_eval_melt_df = pd.melt(sales_eval_df, id_vars=['id','item_id','dept_id','cat_id','store_id','state_id'],\n",
    "                       var_name='d',value_name='sales')\n",
    "\n",
    "#changing the dtype of object to category in order to reduce the size of dataframe\n",
    "for col in sales_eval_melt_df.columns[:6]:\n",
    "    sales_eval_melt_df[col] = sales_eval_melt_df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a single dataframe\n",
    "sales_eval_melt_df = sales_eval_melt_df.merge(calendar_df,  on='d', how='left')\n",
    "sales_eval_melt_df = sales_eval_melt_df.merge(prices_df, on=['store_id','item_id','wm_yr_wk'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre processing missing values of prices by transforming with mean price of that id\n",
    "sales_eval_melt_df['sell_price'].fillna(sales_eval_melt_df.groupby('id')['sell_price'].transform('mean'),inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Lag features </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:26<00:00,  2.99s/it]\n"
     ]
    }
   ],
   "source": [
    "#creating lag features such that the for a product on current day it gets it's sales upto 3 months prior.\n",
    "shifting = 28 #shift period in order to account for 28 days to forecast\n",
    "for i in tqdm(range(9)): #num of weeks to shift here 8 weeks we consider\n",
    "    sales_eval_melt_df['lag_'+str(shifting+(7*i))] = sales_eval_melt_df.groupby('id')['sales'].shift(shifting+(7*i)).astype(np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Rolling features </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [6:21:35<00:00, 4579.14s/it]  \n"
     ]
    }
   ],
   "source": [
    "#creating constant shift rolling agg features\n",
    "for i in tqdm([7,14,28,35,60]):\n",
    "    sales_eval_melt_df['rolling_mean_'+str(i)] =  sales_eval_melt_df.groupby(['id'])['lag_28'].transform(lambda x: x.rolling(i).mean())\n",
    "    sales_eval_melt_df['rolling_median_'+str(i)] =  sales_eval_melt_df.groupby(['id'])['lag_28'].transform(lambda x: x.rolling(i).median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_eval_melt_df.to_pickle(\"sales_eval_melt_df\") #store the dataframe into disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_eval_melt_df = pd.read_pickle(\"sales_eval_melt_df\") #load the dataframe from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Calender features</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing dtype of calender features to category\n",
    "cal_cols = ['event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI']\n",
    "for col in cal_cols:\n",
    "    sales_eval_melt_df[col] = sales_eval_melt_df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_eval_melt_df['date'] = pd.to_datetime(sales_eval_melt_df['date'])\n",
    "#each day of the month\n",
    "sales_eval_melt_df['day_of_month'] = sales_eval_melt_df['date'].dt.day.astype(np.int8)\n",
    "#changing year value as 0 for 2011 and 1 for 2012 .... 5 for 2016\n",
    "sales_eval_melt_df['year'] = (sales_eval_melt_df['year'] - sales_eval_melt_df['year'].min()).astype(np.int8)\n",
    "#week number of a day in a month ex: 29th in January corresponds to 5th week of January\n",
    "sales_eval_melt_df['week_no_inmonth'] = sales_eval_melt_df['day_of_month'].apply(lambda x: math.ceil(x/7)).astype(np.int8)\n",
    "#checking if the day is weekend or not\n",
    "sales_eval_melt_df['is_weekend'] = (sales_eval_melt_df['wday']<=2).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_eval_melt_df.to_pickle(\"sales_eval_melt_calfadd_df\") #store the final feature engineered dataframe to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_eval_melt_df = pd.read_pickle(\"sales_melt_calfadd_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>sales</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>...</th>\n",
       "      <th>rolling_median_14</th>\n",
       "      <th>rolling_mean_28</th>\n",
       "      <th>rolling_median_28</th>\n",
       "      <th>rolling_mean_35</th>\n",
       "      <th>rolling_median_35</th>\n",
       "      <th>rolling_mean_60</th>\n",
       "      <th>rolling_median_60</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>week_no_inmonth</th>\n",
       "      <th>is_weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id    d  sales       date  wm_yr_wk  ... rolling_median_14  \\\n",
       "0       CA  d_1   0.00 2011-01-29     11101  ...               nan   \n",
       "1       CA  d_1   0.00 2011-01-29     11101  ...               nan   \n",
       "\n",
       "   rolling_mean_28  rolling_median_28  rolling_mean_35 rolling_median_35  \\\n",
       "0              nan                nan              nan               nan   \n",
       "1              nan                nan              nan               nan   \n",
       "\n",
       "  rolling_mean_60 rolling_median_60 day_of_month week_no_inmonth is_weekend  \n",
       "0             nan               nan           29               5          1  \n",
       "1             nan               nan           29               5          1  \n",
       "\n",
       "[2 rows x 44 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_eval_melt_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pre-processing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the dtype to category for these columns in order to process the columns with label encoding\n",
    "cat_cols = ['id','item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2','snap_CA','snap_TX','snap_WI']\n",
    "for col in cat_cols:\n",
    "    sales_eval_melt_df[col] = sales_eval_melt_df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method which return the label encoded columns\n",
    "def label_encoding(df,cols):\n",
    "    for col in cols:\n",
    "        lenc = LabelEncoder()\n",
    "        df[col] = lenc.fit_transform(df[col].astype(str))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enc = label_encoding(sales_eval_melt_df,cat_cols) #transforming the categorical columns to label encoded columns\n",
    "df_enc['d'] = df_enc['d'].apply(lambda x: x.split('_')[1]).astype(np.int16) #splitting the values of 'd' comlumn to take only the day number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final dataframe after pre-processing and feature engineering we are taking last 2 years data to train the ML model\n",
    "df_final = df_enc.loc[pd.to_datetime(df_enc['date'].dt.date) >= '2014-01-02']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"df_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_final.drop(['sales','date','weekday','wm_yr_wk'],axis=1)\n",
    "X.reset_index(drop=True,inplace=True)\n",
    "y = df_final['sales']\n",
    "y.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evaluation metric - WRMSSE</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/dhananjay3/wrmsse-evaluator-with-extra-features\n",
    "#here we are transforming the 30490 timeseries into 42840 time-series by grouping based on the 12 level hirearchy\n",
    "def convert_to_42840(df, cols, groupbys):\n",
    "    series_gen = {}\n",
    "    for i, grp in enumerate(groupbys):\n",
    "        #grop by corresponding group and calculating aggregate sales of each day\n",
    "        tmp = df.groupby(grp)[cols].sum()\n",
    "        #storing the aggregate sale values of each corresponding group\n",
    "        for j in range(len(tmp)):\n",
    "            series_gen[gen_series_name(tmp.index[j])] = tmp.iloc[j].values\n",
    "    return pd.DataFrame(series_gen).T #creating a dataframe of each corresponding group and aggregate sales each day i.e., transformed into 42840 sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this method return the name of each group\n",
    "def gen_series_name(name):\n",
    "    if isinstance(name, str) | isinstance(name, int):\n",
    "        return str(name)\n",
    "    else:\n",
    "        return \"__\".join(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we are computing weights using the last 28 day sales of train data and their prices\n",
    "def compute_weights(train_df,valid_df,weight_cols,groupbys,fix_cols):\n",
    "    weights_map = {}\n",
    "    weight_df = train_df[[\"item_id\", \"store_id\"] + weight_cols]\n",
    "    weight_df = pd.melt(weight_df,id_vars=[\"item_id\", \"store_id\"],var_name='d',value_name='sales')\n",
    "    weight_df = weight_df.merge(calendar_df[['wm_yr_wk','d']], on='d', how='left')\n",
    "    weight_df = weight_df.merge(prices_df, how=\"left\", on=[\"item_id\", \"store_id\", \"wm_yr_wk\"])\n",
    "    #computing dollar sales \n",
    "    weight_df[\"dollar_sales\"] = weight_df[\"sales\"] * weight_df[\"sell_price\"]\n",
    "    weight_df = weight_df.set_index([\"item_id\", \"store_id\", \"d\"]).unstack(level=2)[\"dollar_sales\"]\n",
    "    weight_df = weight_df.loc[zip(train_df.item_id, train_df.store_id), :].reset_index(drop=True)\n",
    "    weight_df = pd.concat([train_df[fix_cols], weight_df],\n",
    "                          axis=1, sort=False)\n",
    "    #computing the weights for each group keys\n",
    "    for i,grp in enumerate(groupbys):\n",
    "        ser_weight = weight_df.groupby(grp)[weight_cols].sum().sum(axis=1)\n",
    "        ser_weight = ser_weight / ser_weight.sum()\n",
    "        for j in range(len(ser_weight)):\n",
    "            weights_map[gen_series_name(ser_weight.index[j])] = np.array([ser_weight.iloc[j]])\n",
    "    weights = pd.DataFrame(weights_map).T / len(groupbys) #creating a dataframe with weights corresponding to each group keys of 42840 hierachical time-series\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we compute the rmsse using the true values and predicted values along with train data which is being used to scale\n",
    "#train data is used to scale the squared-error as taking the consecutive difference of each day sales\n",
    "def compute_rmsse(train_df, valid_df, pred_df):\n",
    "    scale_lst = []\n",
    "    for i in range(len(train_df)):\n",
    "        val = train_df.iloc[i].values\n",
    "        # to consider the periods following the first non-zero demand observed for the series under evaluation.\n",
    "        val = val[np.argmax(val!=0):]\n",
    "        #to scale the squared-error as taking the consecutive difference of each day sales\n",
    "        scale = ((val[1:] - val[:-1]) ** 2).mean()\n",
    "        #storing the scale value corresponding to each time series\n",
    "        scale_lst.append(scale)\n",
    "    scale_arr = np.array(scale_lst)\n",
    "    #computing mean squared error\n",
    "    num = ((pred_df - valid_df)**2).mean(axis=1)\n",
    "    #scaled error i.e., root mean squared scaled error\n",
    "    rmsse = (num/scale_arr).map(np.sqrt)\n",
    "    return rmsse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we return the final score value i.e., WRMSSE\n",
    "def custom_metric(train_df, valid_df, pred_df, weights):\n",
    "    #obtaing RMSSE by calling compute_rmsse function\n",
    "    rmsse = compute_rmsse(train_df, valid_df, pred_df)\n",
    "    #WRMSSE of each 42840 time-series is computed as product of corresponding weights and RMSSE respectively\n",
    "    ser_metric = pd.concat([weights, rmsse], axis=1, sort=False).prod(axis=1)\n",
    "    return np.sum(ser_metric) #aggregation of each WRMSSE of 42840 time-series to get the final WRMSSE score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Custom-Ensemble model with hyper-parameter tuning</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this method returns the dataframes splitted based on specified percentage\n",
    "def split_data(X,y,per):\n",
    "    n_rows = int(len(X)*per)\n",
    "    X1 = X.head(n_rows)\n",
    "    y1 = y.head(n_rows)\n",
    "    X2 = X.loc[X.index.difference(X1.index, sort=False)]\n",
    "    y2 = y.loc[y.index.difference(y1.index, sort=False)]\n",
    "    return X1,y1,X2,y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#formulating X_train,X_test & D1,D2 along with their target values\n",
    "X_train,y_train,X_test,y_test = split_data(X,y,0.8)\n",
    "D1_x,D1_y,D2_x,D2_y = split_data(X_train,y_train,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the D1,D2,test dataframes\n",
    "D1_x.to_pickle('D1_x')\n",
    "D1_y.to_pickle('D1_y')\n",
    "D2_x.to_pickle('D2_x')\n",
    "D2_y.to_pickle('D2_y')\n",
    "X_test.to_pickle('X_test')\n",
    "y_test.to_pickle('y_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the D1,D2,test dataframes\n",
    "D1_x = pd.read_pickle(\"D1_x\")\n",
    "D1_y = pd.read_pickle(\"D1_y\")\n",
    "D2_x = pd.read_pickle(\"D2_x\")\n",
    "D2_y = pd.read_pickle(\"D2_y\")\n",
    "X_test = pd.read_pickle(\"X_test\")\n",
    "y_test = pd.read_pickle(\"y_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes of D1,D2,test dataframes (10976400, 40) (10976400, 40) (5488200, 40)\n"
     ]
    }
   ],
   "source": [
    "print('shapes of D1,D2,test dataframes',D1_x.shape,D2_x.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a custom method where we return the predictions of the test data X_test.\n",
    "#we train N base models with sampled with replacement D1 data and using those models \n",
    "#we get the predictions as features from each model for D2 data and train the meta model \n",
    "#with predicted values of D2 based on N models, D2_y\n",
    "#getting predictions of X_test from N base models we get the final predictions from trained meta model.\n",
    "def custom_ensemble(D1_x,D1_y,D2_x,D2_y,X_test,y_test,N,base_learner,metaM):\n",
    "    #defing some of variables to be used dynamically for base models, sample dataframes, features, predictions\n",
    "    sample_D1_x = ['d'+str(i)+'_x' for i in range(1,N+1)]\n",
    "    sample_D1_y = ['d'+str(i)+'_y' for i in range(1,N+1)]\n",
    "    base_models = ['M'+str(i) for i in range(1,N+1)]\n",
    "    preds_D2_x  = ['pred_d2_'+str(i) for i in range(int(D2_x.iloc[0]['d']),int(D2_x.iloc[-1]['d'])+1)]\n",
    "    preds_X_test  = ['pred_X_test_'+str(i) for i in range(int(X_test.iloc[0]['d']),int(X_test.iloc[-1]['d'])+1)]\n",
    "    features_D2_pred = ['D2_f_'+str(i) for i in range(1,N+1)]\n",
    "    features_X_test_pred = ['X_test_f_'+str(i) for i in range(1,N+1)]\n",
    "    D2_pred = pd.DataFrame()\n",
    "    X_test_pred = pd.DataFrame()\n",
    "    #N represents number of base models\n",
    "    for i in tqdm(range(N)):\n",
    "        #getting the sampled with replacement D1_x\n",
    "        sample_D1_x[i] = D1_x.sample(frac=1,replace=True)\n",
    "        #getting the sampled with replacement D1_y\n",
    "        sample_D1_y[i] = D1_y.loc[sample_D1_x[i].index]\n",
    "        #defing each base model\n",
    "        base_models[i] = base_learner\n",
    "        #training each base model\n",
    "        base_models[i].fit(sample_D1_x[i],sample_D1_y[i])\n",
    "        #predicting for all the days of D2_x using traing N base models and using them as features\n",
    "        preds_D2_x[i] = pd.DataFrame()\n",
    "        for j in range(int(D2_x.iloc[0]['d']),int(D2_x.iloc[-1]['d'])+1):\n",
    "            preds_D2_x[i]['d_'+str(j)] = base_models[i].predict(D2_x[D2_x['d']==j])\n",
    "        df1 = pd.melt(preds_D2_x[i],var_name='d',value_name='sales')\n",
    "        #creating dataframe with features as predictions of D2_x obtained from trained N base models\n",
    "        D2_pred[features_D2_pred[i]] = df1['sales'].values\n",
    "        #predicting for all the days of X_test using traing N base models and using them as features\n",
    "        preds_X_test[i] = pd.DataFrame()\n",
    "        for k in range(int(X_test.iloc[0]['d']),int(X_test.iloc[-1]['d'])+1):\n",
    "            preds_X_test[i]['d_'+str(k)] = base_models[i].predict(X_test[X_test['d']==k])\n",
    "        df2 = pd.melt(preds_X_test[i],var_name='d',value_name='sales')\n",
    "        #creating dataframe with features as predictions of X_test obtained from trained N base models\n",
    "        X_test_pred[features_X_test_pred[i]] = df2['sales'].values\n",
    "    #training meta-model with D2_pred,D2_y\n",
    "    meta_model = metaM\n",
    "    #fit the model\n",
    "    meta_model.fit(D2_pred.values,D2_y.values)\n",
    "    #getting the predictions for X_test_pred from trained meta-model\n",
    "    meta_model_preds = meta_model.predict(X_test_pred.values)\n",
    "    return meta_model_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Tuning with different base-learners and meta-model combinations</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [01:19<11:53, 79.32s/it]\u001b[A\n",
      " 20%|██        | 2/10 [02:37<10:31, 78.94s/it]\u001b[A\n",
      " 30%|███       | 3/10 [03:56<09:11, 78.86s/it]\u001b[A\n",
      " 40%|████      | 4/10 [05:14<07:53, 78.86s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [06:34<06:35, 79.02s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [07:52<05:15, 78.92s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [09:12<03:57, 79.03s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [10:32<02:38, 79.42s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [11:51<01:19, 79.27s/it]\u001b[A\n",
      "100%|██████████| 10/10 [13:11<00:00, 79.19s/it]\u001b[A\n",
      " 25%|██▌       | 1/4 [25:12<1:15:37, 1512.61s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [01:25<12:45, 85.08s/it]\u001b[A\n",
      " 20%|██        | 2/10 [02:47<11:14, 84.37s/it]\u001b[A\n",
      " 30%|███       | 3/10 [04:10<09:46, 83.75s/it]\u001b[A\n",
      " 40%|████      | 4/10 [05:33<08:21, 83.58s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [06:56<06:56, 83.37s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [08:20<05:34, 83.73s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [09:43<04:10, 83.45s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [11:05<02:46, 83.01s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [12:29<01:23, 83.19s/it]\u001b[A\n",
      "100%|██████████| 10/10 [13:53<00:00, 83.30s/it]\u001b[A\n",
      " 50%|█████     | 2/4 [39:51<44:04, 1322.37s/it]  \n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [24:05<3:36:52, 1445.81s/it]\u001b[A\n",
      " 20%|██        | 2/10 [47:47<3:11:49, 1438.71s/it]\u001b[A\n",
      " 30%|███       | 3/10 [1:11:44<2:47:47, 1438.17s/it]\u001b[A\n",
      " 40%|████      | 4/10 [1:35:19<2:23:07, 1431.17s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [2:00:44<2:01:35, 1459.17s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [2:25:37<1:37:57, 1469.26s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [2:50:22<1:13:42, 1474.27s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [3:14:00<48:34, 1457.37s/it]  \u001b[A\n",
      " 90%|█████████ | 9/10 [3:37:23<24:01, 1441.04s/it]\u001b[A\n",
      "100%|██████████| 10/10 [4:01:47<00:00, 1450.73s/it]\u001b[A\n",
      " 75%|███████▌  | 3/4 [4:42:28<1:28:12, 5292.78s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [01:36<14:25, 96.19s/it]\u001b[A\n",
      " 20%|██        | 2/10 [03:06<12:35, 94.46s/it]\u001b[A\n",
      " 30%|███       | 3/10 [04:40<11:00, 94.29s/it]\u001b[A\n",
      " 40%|████      | 4/10 [06:19<09:33, 95.57s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [07:51<07:53, 94.73s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [09:35<06:30, 97.53s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [11:07<04:47, 95.84s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [12:39<03:09, 94.52s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [14:09<01:33, 93.19s/it]\u001b[A\n",
      "100%|██████████| 10/10 [15:40<00:00, 94.10s/it]\u001b[A\n",
      "100%|██████████| 4/4 [5:13:23<00:00, 4700.78s/it]  \n"
     ]
    }
   ],
   "source": [
    "groupbys = ('for_all', 'state_id', 'store_id', 'cat_id', 'dept_id',['state_id', 'cat_id'],  \n",
    "            ['state_id', 'dept_id'], ['store_id', 'cat_id'],['store_id', 'dept_id'], 'item_id', \n",
    "            ['item_id', 'state_id'], ['item_id', 'store_id'])\n",
    "\n",
    "train_df = pd.concat([sales_eval_df.loc[:,:'state_id'],sales_eval_df.loc[:,'d_1070':]],axis=1,sort=False)\n",
    "train_df = train_df.iloc[:,:-28]\n",
    "valid_df = sales_eval_df.iloc[:,-28:].copy()\n",
    "train_d_cols = [col for col in train_df.columns if col.startswith('d_')]\n",
    "weight_cols = train_df.iloc[:,-28:].columns.tolist()\n",
    "train_df['for_all'] = \"all\" #for level 1 aggregation\n",
    "fixed_cols = [col for col in train_df.columns if not col.startswith('d_')]\n",
    "valid_d_cols = [col for col in valid_df.columns if col.startswith('d_')]\n",
    "if not all([col in valid_df.columns for col in fixed_cols]):\n",
    "    valid_df = pd.concat([train_df[fixed_cols],valid_df],axis=1,sort=False)\n",
    "weight_df = compute_weights(train_df,valid_df,weight_cols,groupbys,fixed_cols)\n",
    "train_42840_df = convert_to_42840(train_df, train_d_cols, groupbys)\n",
    "valid_42840_df = convert_to_42840(valid_df, valid_d_cols, groupbys)\n",
    "\n",
    "#getting the X_test predictions for different base-learners and meta-models\n",
    "model_combs = [(DecisionTreeRegressor(max_depth=10,max_features=10,random_state=42),\n",
    "                 xgb.XGBRegressor(n_estimators=50,learning_rate=0.05,max_depth=10,n_jobs=-1,\n",
    "                                  colsample_bytree=0.3,subsample=1,random_state=42)),\n",
    "                 (DecisionTreeRegressor(max_depth=10,max_features=10,random_state=42),\n",
    "                  lgb.LGBMRegressor(num_leaves=125,n_estimators=100,learning_rate=0.075,n_jobs=-1)),\n",
    "                 (xgb.XGBRegressor(n_estimators=50,learning_rate=0.05,max_depth=10,n_jobs=-1,\n",
    "                                  colsample_bytree=0.3,subsample=1,random_state=42),\n",
    "                 lgb.LGBMRegressor(num_leaves=125,n_estimators=100,learning_rate=0.075,n_jobs=-1)),\n",
    "                 (lgb.LGBMRegressor(num_leaves=125,n_estimators=100,learning_rate=0.075,n_jobs=-1),\n",
    "                 xgb.XGBRegressor(n_estimators=50,learning_rate=0.05,max_depth=10,n_jobs=-1,\n",
    "                                  colsample_bytree=0.3,subsample=1,random_state=42))]\n",
    "WRMSSE_custom_model_combs = {}\n",
    "for i in tqdm(range(len(model_combs))):\n",
    "    base_learner = model_combs[i][0]\n",
    "    metaM = model_combs[i][1]\n",
    "    custom_ensemle_predictions = custom_ensemble(D1_x,D1_y,D2_x,D2_y,X_test,y_test,10,base_learner,metaM)\n",
    "\n",
    "    #slicing the predictions such that to get each day predictions of all the products of test data\n",
    "    start = 0\n",
    "    t = int(X_test.iloc[0]['d'])\n",
    "    custom_ensemle_predictions_df = pd.DataFrame()\n",
    "    while start < len(custom_ensemle_predictions):\n",
    "        end = start + 30490\n",
    "        custom_ensemle_predictions_df['d_'+str(t)] = custom_ensemle_predictions[start:end]\n",
    "        start = end\n",
    "        t = t+1\n",
    "\n",
    "    forecast_horizon_pred =  custom_ensemle_predictions_df.iloc[:,-56:-28]\n",
    "    forecast_horizon_pred = pd.concat([valid_df[fixed_cols], forecast_horizon_pred],axis=1,sort=False)\n",
    "    #prediction data transformed from 30490 timeseries to 42840 hirerachichal time-series\n",
    "    pred_42840_df = convert_to_42840(forecast_horizon_pred,valid_d_cols,groupbys)\n",
    "    #Computed WRMSSE for each predictions based on hyper-parameters\n",
    "    WRMSSE_custom_model_combs[i] = custom_metric(train_42840_df,valid_42840_df,pred_42840_df,weight_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRMSSE for different base-learners and meta-model combination is :\n",
      " {0: 0.7727196527957321, 1: 0.7447778931206561, 2: 0.7058861770893176, 3: 0.7641068479229063}\n"
     ]
    }
   ],
   "source": [
    "print('WRMSSE for different base-learners and meta-model combination is :\\n',WRMSSE_custom_model_combs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(WRMSSE_custom_model_combs, open('WRMSSE_custom_model_combs', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> After trying with different base-learners and meta-model combinations we observe that XGBRegressor as base-learner and LGBMRegressor as meta-model gives lower WRMSSE benchmark. </h4>\n",
    "<h5>Fixing base-learner to be XGBRegressor and meta-model to be LGBMRegressor and tuning the number of base-learners 'N' to improve WRMSSE</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|▊         | 1/12 [16:32<3:02:00, 992.79s/it]\u001b[A\n",
      " 17%|█▋        | 2/12 [33:11<2:45:45, 994.59s/it]\u001b[A\n",
      " 25%|██▌       | 3/12 [49:37<2:28:47, 991.91s/it]\u001b[A\n",
      " 33%|███▎      | 4/12 [1:05:47<2:11:23, 985.45s/it]\u001b[A\n",
      " 42%|████▏     | 5/12 [1:22:42<1:56:00, 994.34s/it]\u001b[A\n",
      " 50%|█████     | 6/12 [1:32:04<1:26:27, 864.55s/it]\u001b[A\n",
      " 58%|█████▊    | 7/12 [1:41:41<1:04:51, 778.32s/it]\u001b[A\n",
      " 67%|██████▋   | 8/12 [1:54:07<51:14, 768.71s/it]  \u001b[A\n",
      " 75%|███████▌  | 9/12 [2:10:45<41:51, 837.27s/it]\u001b[A\n",
      " 83%|████████▎ | 10/12 [2:21:29<25:58, 779.48s/it]\u001b[A\n",
      " 92%|█████████▏| 11/12 [2:35:33<13:18, 798.86s/it]\u001b[A\n",
      "100%|██████████| 12/12 [2:51:52<00:00, 859.39s/it]\u001b[A\n",
      " 33%|███▎      | 1/3 [2:53:26<5:46:52, 10406.33s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRMSSE score is:  {12: 0.7054210382859196}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  7%|▋         | 1/15 [16:38<3:52:56, 998.32s/it]\u001b[A\n",
      " 13%|█▎        | 2/15 [33:13<3:36:04, 997.23s/it]\u001b[A\n",
      " 20%|██        | 3/15 [48:29<3:14:37, 973.13s/it]\u001b[A\n",
      " 27%|██▋       | 4/15 [58:14<2:37:01, 856.48s/it]\u001b[A\n",
      " 33%|███▎      | 5/15 [1:08:19<2:10:11, 781.12s/it]\u001b[A\n",
      " 40%|████      | 6/15 [1:17:50<1:47:41, 717.94s/it]\u001b[A\n",
      " 47%|████▋     | 7/15 [1:27:10<1:29:26, 670.81s/it]\u001b[A\n",
      " 53%|█████▎    | 8/15 [1:36:49<1:15:01, 643.01s/it]\u001b[A\n",
      " 60%|██████    | 9/15 [1:46:16<1:02:01, 620.31s/it]\u001b[A\n",
      " 67%|██████▋   | 10/15 [1:55:49<50:30, 606.11s/it] \u001b[A\n",
      " 73%|███████▎  | 11/15 [2:05:18<39:39, 594.96s/it]\u001b[A\n",
      " 80%|████████  | 12/15 [2:14:41<29:16, 585.57s/it]\u001b[A\n",
      " 87%|████████▋ | 13/15 [2:24:25<19:29, 584.85s/it]\u001b[A\n",
      " 93%|█████████▎| 14/15 [2:33:53<09:39, 579.94s/it]\u001b[A\n",
      "100%|██████████| 15/15 [2:42:59<00:00, 651.99s/it]\u001b[A\n",
      " 67%|██████▋   | 2/3 [5:37:11<2:50:31, 10231.89s/it]\n",
      "  0%|          | 0/18 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRMSSE score is:  {12: 0.7054210382859196, 15: 0.7039885143961281}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▌         | 1/18 [09:59<2:49:46, 599.22s/it]\u001b[A\n",
      " 11%|█         | 2/18 [19:46<2:38:51, 595.74s/it]\u001b[A\n",
      " 17%|█▋        | 3/18 [29:51<2:29:35, 598.37s/it]\u001b[A\n",
      " 22%|██▏       | 4/18 [39:37<2:18:44, 594.61s/it]\u001b[A\n",
      " 28%|██▊       | 5/18 [48:46<2:05:52, 580.99s/it]\u001b[A\n",
      " 33%|███▎      | 6/18 [58:16<1:55:33, 577.81s/it]\u001b[A\n",
      " 39%|███▉      | 7/18 [1:07:49<1:45:38, 576.24s/it]\u001b[A\n",
      " 44%|████▍     | 8/18 [1:17:20<1:35:48, 574.85s/it]\u001b[A\n",
      " 50%|█████     | 9/18 [1:26:44<1:25:44, 571.60s/it]\u001b[A\n",
      " 56%|█████▌    | 10/18 [1:36:02<1:15:38, 567.30s/it]\u001b[A\n",
      " 61%|██████    | 11/18 [1:45:26<1:06:04, 566.34s/it]\u001b[A\n",
      " 67%|██████▋   | 12/18 [1:55:03<56:57, 569.65s/it]  \u001b[A\n",
      " 72%|███████▏  | 13/18 [2:04:44<47:45, 573.03s/it]\u001b[A\n",
      " 78%|███████▊  | 14/18 [2:14:22<38:17, 574.37s/it]\u001b[A\n",
      " 83%|████████▎ | 15/18 [2:24:14<28:59, 579.70s/it]\u001b[A\n",
      " 89%|████████▉ | 16/18 [2:34:26<19:38, 589.38s/it]\u001b[A\n",
      " 94%|█████████▍| 17/18 [2:44:36<09:55, 595.78s/it]\u001b[A\n",
      "100%|██████████| 18/18 [2:54:46<00:00, 582.57s/it]\u001b[A\n",
      "100%|██████████| 3/3 [8:32:48<00:00, 10256.07s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRMSSE score is:  {12: 0.7054210382859196, 15: 0.7039885143961281, 18: 0.7011200983741996}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "groupbys = ('for_all', 'state_id', 'store_id', 'cat_id', 'dept_id',['state_id', 'cat_id'],  \n",
    "            ['state_id', 'dept_id'], ['store_id', 'cat_id'],['store_id', 'dept_id'], 'item_id', \n",
    "            ['item_id', 'state_id'], ['item_id', 'store_id'])\n",
    "\n",
    "train_df = pd.concat([sales_eval_df.loc[:,:'state_id'],sales_eval_df.loc[:,'d_1070':]],axis=1,sort=False)\n",
    "train_df = train_df.iloc[:,:-28]\n",
    "valid_df = sales_eval_df.iloc[:,-28:].copy()\n",
    "train_d_cols = [col for col in train_df.columns if col.startswith('d_')]\n",
    "weight_cols = train_df.iloc[:,-28:].columns.tolist()\n",
    "train_df['for_all'] = \"all\" #for level 1 aggregation\n",
    "fixed_cols = [col for col in train_df.columns if not col.startswith('d_')]\n",
    "valid_d_cols = [col for col in valid_df.columns if col.startswith('d_')]\n",
    "if not all([col in valid_df.columns for col in fixed_cols]):\n",
    "    valid_df = pd.concat([train_df[fixed_cols],valid_df],axis=1,sort=False)\n",
    "weight_df = compute_weights(train_df,valid_df,weight_cols,groupbys,fixed_cols)\n",
    "train_42840_df = convert_to_42840(train_df, train_d_cols, groupbys)\n",
    "valid_42840_df = convert_to_42840(valid_df, valid_d_cols, groupbys)\n",
    "\n",
    "#getting the X_test predictions for different base-learners and meta-models\n",
    "#fixing the base-learners and meta-model and tuning the number of base-learners\n",
    "N_list = [12,15,18]\n",
    "WRMSSE_custom_model_N1 = {}\n",
    "for n in tqdm(N_list):\n",
    "    base_learner = xgb.XGBRegressor(n_estimators=50,learning_rate=0.05,max_depth=10,n_jobs=-1,\n",
    "                                  colsample_bytree=0.3,subsample=1,random_state=42)\n",
    "    metaM = lgb.LGBMRegressor(num_leaves=125,n_estimators=100,learning_rate=0.075,n_jobs=-1)\n",
    "    custom_ensemle_predictions = custom_ensemble(D1_x,D1_y,D2_x,D2_y,X_test,y_test,n,base_learner,metaM)\n",
    "\n",
    "    #slicing the predictions such that to get each day predictions of all the products of test data\n",
    "    start = 0\n",
    "    t = int(X_test.iloc[0]['d'])\n",
    "    custom_ensemle_predictions_df = pd.DataFrame()\n",
    "    while start < len(custom_ensemle_predictions):\n",
    "        end = start + 30490\n",
    "        custom_ensemle_predictions_df['d_'+str(t)] = custom_ensemle_predictions[start:end]\n",
    "        start = end\n",
    "        t = t+1\n",
    "\n",
    "    forecast_horizon_pred =  custom_ensemle_predictions_df.iloc[:,-56:-28]\n",
    "    forecast_horizon_pred = pd.concat([valid_df[fixed_cols], forecast_horizon_pred],axis=1,sort=False)\n",
    "    #prediction data transformed from 30490 timeseries to 42840 hirerachichal time-series\n",
    "    pred_42840_df = convert_to_42840(forecast_horizon_pred,valid_d_cols,groupbys)\n",
    "    #Computed WRMSSE for each predictions based on hyper-parameters\n",
    "    WRMSSE_custom_model_N1[n] = custom_metric(train_42840_df,valid_42840_df,pred_42840_df,weight_df)\n",
    "    print('WRMSSE score is: ',WRMSSE_custom_model_N1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRMSSE for different number base-learners is :\n",
      " {12: 0.7054210382859196, 15: 0.7039885143961281, 18: 0.7011200983741996}\n"
     ]
    }
   ],
   "source": [
    "print('WRMSSE for different number base-learners is :\\n',WRMSSE_custom_model_N1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Observation</h4>\n",
    "1. Tuning the number of base-learners we observe that for 'N'(number of base-learners) = 18, gives lower WRMSSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(WRMSSE_custom_model_N1, open('WRMSSE_custom_model_N1', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Getting the test predictions from the custom ensemble with N=18</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d_1790</th>\n",
       "      <th>d_1791</th>\n",
       "      <th>d_1792</th>\n",
       "      <th>d_1793</th>\n",
       "      <th>d_1794</th>\n",
       "      <th>d_1795</th>\n",
       "      <th>d_1796</th>\n",
       "      <th>d_1797</th>\n",
       "      <th>d_1798</th>\n",
       "      <th>d_1799</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1960</th>\n",
       "      <th>d_1961</th>\n",
       "      <th>d_1962</th>\n",
       "      <th>d_1963</th>\n",
       "      <th>d_1964</th>\n",
       "      <th>d_1965</th>\n",
       "      <th>d_1966</th>\n",
       "      <th>d_1967</th>\n",
       "      <th>d_1968</th>\n",
       "      <th>d_1969</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.52</td>\n",
       "      <td>...</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.26</td>\n",
       "      <td>1.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   d_1790  d_1791  d_1792  d_1793  d_1794  d_1795  d_1796  d_1797  d_1798  \\\n",
       "0    0.51    0.42    0.34    0.61    0.67    0.56    0.51    0.57    0.51   \n",
       "1    0.40    0.40    0.30    0.50    0.49    0.38    0.36    0.32    0.31   \n",
       "\n",
       "   d_1799  ...  d_1960  d_1961  d_1962  d_1963  d_1964  d_1965  d_1966  \\\n",
       "0    0.52  ...    1.11    1.49    1.35    1.00    0.98    0.91    0.98   \n",
       "1    0.30  ...    0.14    0.23    0.32    0.29    0.34    0.33    0.31   \n",
       "\n",
       "   d_1967  d_1968  d_1969  \n",
       "0    1.11    1.26    1.28  \n",
       "1    0.33    0.35    0.34  \n",
       "\n",
       "[2 rows x 180 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_ensemle_predictions_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>for_all</th>\n",
       "      <th>d_1790</th>\n",
       "      <th>d_1791</th>\n",
       "      <th>d_1792</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1960</th>\n",
       "      <th>d_1961</th>\n",
       "      <th>d_1962</th>\n",
       "      <th>d_1963</th>\n",
       "      <th>d_1964</th>\n",
       "      <th>d_1965</th>\n",
       "      <th>d_1966</th>\n",
       "      <th>d_1967</th>\n",
       "      <th>d_1968</th>\n",
       "      <th>d_1969</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.34</td>\n",
       "      <td>...</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.26</td>\n",
       "      <td>1.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 187 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id for_all  d_1790  d_1791  d_1792  ...  d_1960  d_1961  d_1962  \\\n",
       "0       CA     all    0.51    0.42    0.34  ...    1.11    1.49    1.35   \n",
       "1       CA     all    0.40    0.40    0.30  ...    0.14    0.23    0.32   \n",
       "\n",
       "   d_1963  d_1964  d_1965  d_1966  d_1967  d_1968  d_1969  \n",
       "0    1.00    0.98    0.91    0.98    1.11    1.26    1.28  \n",
       "1    0.29    0.34    0.33    0.31    0.33    0.35    0.34  \n",
       "\n",
       "[2 rows x 187 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_ensemle_predictions_df = pd.concat([valid_df[fixed_cols], custom_ensemle_predictions_df],axis=1,sort=False)\n",
    "custom_ensemle_predictions_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_ensemle_predictions_df.to_csv('custom_ensemle_predictions_df_N18.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>for_all</th>\n",
       "      <th>d_1790</th>\n",
       "      <th>d_1791</th>\n",
       "      <th>d_1792</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1960</th>\n",
       "      <th>d_1961</th>\n",
       "      <th>d_1962</th>\n",
       "      <th>d_1963</th>\n",
       "      <th>d_1964</th>\n",
       "      <th>d_1965</th>\n",
       "      <th>d_1966</th>\n",
       "      <th>d_1967</th>\n",
       "      <th>d_1968</th>\n",
       "      <th>d_1969</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.34</td>\n",
       "      <td>...</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.26</td>\n",
       "      <td>1.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>1.42</td>\n",
       "      <td>1.46</td>\n",
       "      <td>1.16</td>\n",
       "      <td>...</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.97</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.39</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.92</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.66</td>\n",
       "      <td>...</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.27</td>\n",
       "      <td>1.34</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 187 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id for_all  d_1790  d_1791  d_1792  ...  d_1960  d_1961  d_1962  \\\n",
       "0       CA     all    0.51    0.42    0.34  ...    1.11    1.49    1.35   \n",
       "1       CA     all    0.40    0.40    0.30  ...    0.14    0.23    0.32   \n",
       "2       CA     all    0.74    0.60    0.63  ...    0.68    0.80    0.83   \n",
       "3       CA     all    1.42    1.46    1.16  ...    1.63    2.05    1.97   \n",
       "4       CA     all    1.08    1.04    0.66  ...    1.53    1.83    1.85   \n",
       "\n",
       "   d_1963  d_1964  d_1965  d_1966  d_1967  d_1968  d_1969  \n",
       "0    1.00    0.98    0.91    0.98    1.11    1.26    1.28  \n",
       "1    0.29    0.34    0.33    0.31    0.33    0.35    0.34  \n",
       "2    0.63    0.60    0.54    0.64    0.75    0.79    0.78  \n",
       "3    1.57    1.39    1.41    1.51    1.50    1.92    2.31  \n",
       "4    1.51    1.41    1.30    1.27    1.34    1.58    1.51  \n",
       "\n",
       "[5 rows x 187 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_ensemle_predictions_df_N18 = pd.read_csv('custom_ensemle_predictions_df_N18.csv')\n",
    "custom_ensemle_predictions_df_N18.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Getting the extact validation days(1914-1941) predictions and evaluation days(1942-1969) predictions from custom_ensemble model's test predictions forecasted sales from days(1790-1969) and making into correct submission format to get the private leaderboard score </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_val = custom_ensemle_predictions_df_N18[['id']]\n",
    "#validation predictions from days 1914-1941\n",
    "for i in range(28):\n",
    "    submit_val['F'+str(i+1)] = custom_ensemle_predictions_df_N18['d_'+str(1914+i)]\n",
    "submit_val['id'] =  submit_val['id'].apply(lambda x: x.replace('evaluation','validation'))\n",
    "submit_eval = submit_val.copy()\n",
    "#evaluation predictions from days 1942-1969\n",
    "for i in range(28):\n",
    "    submit_eval['F'+str(i+1)] = custom_ensemle_predictions_df_N18['d_'+str(1942+i)]\n",
    "submit_eval[\"id\"] = submit_eval[\"id\"].apply(lambda x: x.replace('validation','evaluation'))\n",
    "submit_custom_ensemble_N18 = submit_val.append(submit_eval).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_custom_ensemble_N18.to_csv(\"submit_custom_ensemble_N18.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>...</th>\n",
       "      <th>F19</th>\n",
       "      <th>F20</th>\n",
       "      <th>F21</th>\n",
       "      <th>F22</th>\n",
       "      <th>F23</th>\n",
       "      <th>F24</th>\n",
       "      <th>F25</th>\n",
       "      <th>F26</th>\n",
       "      <th>F27</th>\n",
       "      <th>F28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.90</td>\n",
       "      <td>...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>1.99</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.19</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.54</td>\n",
       "      <td>...</td>\n",
       "      <td>1.74</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2.48</td>\n",
       "      <td>1.88</td>\n",
       "      <td>1.63</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.42</td>\n",
       "      <td>2.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.32</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.13</td>\n",
       "      <td>...</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id   F1   F2   F3   F4   F5   F6   F7   F8   F9  \\\n",
       "0  HOBBIES_1_001_CA_1_validation 0.97 0.83 0.82 0.79 0.86 1.02 1.02 0.79 0.90   \n",
       "1  HOBBIES_1_002_CA_1_validation 0.37 0.36 0.36 0.36 0.30 0.35 0.33 0.28 0.27   \n",
       "2  HOBBIES_1_003_CA_1_validation 0.38 0.38 0.37 0.37 0.37 0.41 0.46 0.34 0.33   \n",
       "3  HOBBIES_1_004_CA_1_validation 1.99 1.75 1.50 1.47 1.60 1.95 2.19 1.59 1.54   \n",
       "4  HOBBIES_1_005_CA_1_validation 0.93 0.89 0.99 1.11 1.05 1.32 1.58 1.17 1.13   \n",
       "\n",
       "   ...  F19  F20  F21  F22  F23  F24  F25  F26  F27  F28  \n",
       "0  ... 0.95 1.30 1.17 1.00 0.92 0.86 0.93 0.99 1.23 1.17  \n",
       "1  ... 0.19 0.24 0.24 0.18 0.18 0.22 0.21 0.18 0.24 0.22  \n",
       "2  ... 0.42 0.65 0.67 0.56 0.58 0.63 0.56 0.62 0.78 0.77  \n",
       "3  ... 1.74 2.10 2.48 1.88 1.63 1.49 1.49 1.67 2.42 2.36  \n",
       "4  ... 1.09 1.28 1.29 0.95 0.92 0.95 0.95 1.06 1.30 1.52  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_custom_ensemble_N18.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>...</th>\n",
       "      <th>F19</th>\n",
       "      <th>F20</th>\n",
       "      <th>F21</th>\n",
       "      <th>F22</th>\n",
       "      <th>F23</th>\n",
       "      <th>F24</th>\n",
       "      <th>F25</th>\n",
       "      <th>F26</th>\n",
       "      <th>F27</th>\n",
       "      <th>F28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60975</th>\n",
       "      <td>FOODS_3_823_WI_3_evaluation</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60976</th>\n",
       "      <td>FOODS_3_824_WI_3_evaluation</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60977</th>\n",
       "      <td>FOODS_3_825_WI_3_evaluation</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60978</th>\n",
       "      <td>FOODS_3_826_WI_3_evaluation</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.21</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>...</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.27</td>\n",
       "      <td>1.32</td>\n",
       "      <td>1.18</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.21</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60979</th>\n",
       "      <td>FOODS_3_827_WI_3_evaluation</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.86</td>\n",
       "      <td>...</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.18</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.18</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1.22</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.26</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id   F1   F2   F3   F4   F5   F6   F7   F8  \\\n",
       "60975  FOODS_3_823_WI_3_evaluation 0.28 0.26 0.28 0.33 0.38 0.40 0.38 0.30   \n",
       "60976  FOODS_3_824_WI_3_evaluation 0.43 0.32 0.35 0.46 0.46 0.42 0.38 0.31   \n",
       "60977  FOODS_3_825_WI_3_evaluation 0.73 0.71 0.76 0.74 0.77 1.01 1.02 0.78   \n",
       "60978  FOODS_3_826_WI_3_evaluation 0.96 1.06 0.96 0.94 1.05 1.21 1.24 1.00   \n",
       "60979  FOODS_3_827_WI_3_evaluation 0.86 0.89 0.94 0.87 0.77 1.00 1.01 0.76   \n",
       "\n",
       "        F9  ...  F19  F20  F21  F22  F23  F24  F25  F26  F27  F28  \n",
       "60975 0.40  ... 0.50 0.55 0.71 0.54 0.58 0.62 0.55 0.51 0.66 0.71  \n",
       "60976 0.33  ... 0.30 0.35 0.33 0.30 0.29 0.29 0.32 0.29 0.34 0.34  \n",
       "60977 0.75  ... 0.81 0.92 0.93 0.82 0.74 0.77 0.67 0.75 0.89 0.99  \n",
       "60978 0.98  ... 1.07 1.27 1.32 1.18 1.35 1.21 1.14 1.25 1.49 1.43  \n",
       "60979 0.86  ... 1.13 1.18 1.13 1.18 1.28 1.22 1.15 1.26 1.66 1.63  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_custom_ensemble_N18.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Pre-training the best model and saving to disk</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a custom method where we return the predictions of the test data X_test.\n",
    "#we train N base models with sampled with replacement D1 data and using those models \n",
    "#we get the predictions as features from each model for D2 data and train the meta model \n",
    "#with predicted values of D2 based on N models, D2_y\n",
    "#getting predictions of X_test from N base models we get the final predictions from trained meta model.\n",
    "def custom_ensemble(D1_x,D1_y,D2_x,D2_y,X_test,y_test,N,base_learner,metaM):\n",
    "    #defing some of variables to be used dynamically for base models, sample dataframes, features, predictions\n",
    "    sample_D1_x = ['d'+str(i)+'_x' for i in range(1,N+1)]\n",
    "    sample_D1_y = ['d'+str(i)+'_y' for i in range(1,N+1)]\n",
    "    best_base_models = ['M'+str(i) for i in range(1,N+1)]\n",
    "    preds_D2_x  = ['pred_d2_'+str(i) for i in range(int(D2_x.iloc[0]['d']),int(D2_x.iloc[-1]['d'])+1)]\n",
    "    preds_X_test  = ['pred_X_test_'+str(i) for i in range(int(X_test.iloc[0]['d']),int(X_test.iloc[-1]['d'])+1)]\n",
    "    features_D2_pred = ['D2_f_'+str(i) for i in range(1,N+1)]\n",
    "    features_X_test_pred = ['X_test_f_'+str(i) for i in range(1,N+1)]\n",
    "    D2_pred = pd.DataFrame()\n",
    "    X_test_pred = pd.DataFrame()\n",
    "    #N represents number of base models\n",
    "    for i in tqdm(range(N)):\n",
    "        #getting the sampled with replacement D1_x\n",
    "        sample_D1_x[i] = D1_x.sample(frac=1,replace=True)\n",
    "        #getting the sampled with replacement D1_y\n",
    "        sample_D1_y[i] = D1_y.loc[sample_D1_x[i].index]\n",
    "        #defing each base model\n",
    "        file_name = best_base_models[i]+'.pkl'\n",
    "        best_base_models[i] = base_learner\n",
    "        #training each base model\n",
    "        best_base_models[i].fit(sample_D1_x[i],sample_D1_y[i])\n",
    "        #saving each best base-learners into disk\n",
    "        joblib.dump(best_base_models[i],file_name)\n",
    "        #predicting for all the days of D2_x using traing N base models and using them as features\n",
    "        preds_D2_x[i] = pd.DataFrame()\n",
    "        for j in range(int(D2_x.iloc[0]['d']),int(D2_x.iloc[-1]['d'])+1):\n",
    "            preds_D2_x[i]['d_'+str(j)] = best_base_models[i].predict(D2_x[D2_x['d']==j])\n",
    "        df1 = pd.melt(preds_D2_x[i],var_name='d',value_name='sales')\n",
    "        #creating dataframe with features as predictions of D2_x obtained from trained N base models\n",
    "        D2_pred[features_D2_pred[i]] = df1['sales'].values\n",
    "        #predicting for all the days of X_test using traing N base models and using them as features\n",
    "        preds_X_test[i] = pd.DataFrame()\n",
    "        for k in range(int(X_test.iloc[0]['d']),int(X_test.iloc[-1]['d'])+1):\n",
    "            preds_X_test[i]['d_'+str(k)] = best_base_models[i].predict(X_test[X_test['d']==k])\n",
    "        df2 = pd.melt(preds_X_test[i],var_name='d',value_name='sales')\n",
    "        #creating dataframe with features as predictions of X_test obtained from trained N base models\n",
    "        X_test_pred[features_X_test_pred[i]] = df2['sales'].values\n",
    "    #training meta-model with D2_pred,D2_y\n",
    "    best_meta_model = metaM\n",
    "    #fit the model\n",
    "    best_meta_model.fit(D2_pred.values,D2_y.values)\n",
    "    #saving best meta-model into disk\n",
    "    joblib.dump(best_meta_model,'best_meta_model.pkl')\n",
    "    #getting the predictions for X_test_pred from trained meta-model\n",
    "    meta_model_preds = best_meta_model.predict(X_test_pred.values)\n",
    "    return meta_model_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [3:25:09<00:00, 683.83s/it]  \n"
     ]
    }
   ],
   "source": [
    "groupbys = ('for_all', 'state_id', 'store_id', 'cat_id', 'dept_id',['state_id', 'cat_id'],  \n",
    "            ['state_id', 'dept_id'], ['store_id', 'cat_id'],['store_id', 'dept_id'], 'item_id', \n",
    "            ['item_id', 'state_id'], ['item_id', 'store_id'])\n",
    "\n",
    "train_df = pd.concat([sales_eval_df.loc[:,:'state_id'],sales_eval_df.loc[:,'d_1070':]],axis=1,sort=False)\n",
    "train_df = train_df.iloc[:,:-28]\n",
    "valid_df = sales_eval_df.iloc[:,-28:].copy()\n",
    "train_d_cols = [col for col in train_df.columns if col.startswith('d_')]\n",
    "weight_cols = train_df.iloc[:,-28:].columns.tolist()\n",
    "train_df['for_all'] = \"all\" #for level 1 aggregation\n",
    "fixed_cols = [col for col in train_df.columns if not col.startswith('d_')]\n",
    "valid_d_cols = [col for col in valid_df.columns if col.startswith('d_')]\n",
    "if not all([col in valid_df.columns for col in fixed_cols]):\n",
    "    valid_df = pd.concat([train_df[fixed_cols],valid_df],axis=1,sort=False)\n",
    "weight_df = compute_weights(train_df,valid_df,weight_cols,groupbys,fixed_cols)\n",
    "train_42840_df = convert_to_42840(train_df, train_d_cols, groupbys)\n",
    "valid_42840_df = convert_to_42840(valid_df, valid_d_cols, groupbys)\n",
    "\n",
    "#getting the X_test predictions for different base-learners and meta-models\n",
    "#fixing the base-learners and meta-model and number of base-learners to 18\n",
    "\n",
    "base_learner = xgb.XGBRegressor(n_estimators=50,learning_rate=0.05,max_depth=10,n_jobs=-1,\n",
    "                                  colsample_bytree=0.3,subsample=1,random_state=42)\n",
    "metaM = lgb.LGBMRegressor(num_leaves=125,n_estimators=100,learning_rate=0.075,n_jobs=-1)\n",
    "custom_ensemle_predictions_best = custom_ensemble(D1_x,D1_y,D2_x,D2_y,X_test,y_test,18,base_learner,metaM)\n",
    "\n",
    "#slicing the predictions such that to get each day predictions of all the products of test data\n",
    "start = 0\n",
    "t = int(X_test.iloc[0]['d'])\n",
    "custom_ensemle_predictions_df = pd.DataFrame()\n",
    "while start < len(custom_ensemle_predictions_best):\n",
    "    end = start + 30490\n",
    "    custom_ensemle_predictions_df['d_'+str(t)] = custom_ensemle_predictions_best[start:end]\n",
    "    start = end\n",
    "    t = t+1\n",
    "\n",
    "forecast_horizon_pred =  custom_ensemle_predictions_df.iloc[:,-56:-28]\n",
    "forecast_horizon_pred = pd.concat([valid_df[fixed_cols], forecast_horizon_pred],axis=1,sort=False)\n",
    "#prediction data transformed from 30490 timeseries to 42840 hirerachichal time-series\n",
    "pred_42840_df = convert_to_42840(forecast_horizon_pred,valid_d_cols,groupbys)\n",
    "#Computed WRMSSE for each predictions based on hyper-parameters\n",
    "WRMSSE_custom_model_best = custom_metric(train_42840_df,valid_42840_df,pred_42840_df,weight_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRMSSE of the best custom-built ensemble model is 0.6995\n"
     ]
    }
   ],
   "source": [
    "print('WRMSSE of the best custom-built ensemble model is', round(WRMSSE_custom_model_best,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_ensemle_predictions_df.to_csv('custom_ensemle_predictions_df_best.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d_1790</th>\n",
       "      <th>d_1791</th>\n",
       "      <th>d_1792</th>\n",
       "      <th>d_1793</th>\n",
       "      <th>d_1794</th>\n",
       "      <th>d_1795</th>\n",
       "      <th>d_1796</th>\n",
       "      <th>d_1797</th>\n",
       "      <th>d_1798</th>\n",
       "      <th>d_1799</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1960</th>\n",
       "      <th>d_1961</th>\n",
       "      <th>d_1962</th>\n",
       "      <th>d_1963</th>\n",
       "      <th>d_1964</th>\n",
       "      <th>d_1965</th>\n",
       "      <th>d_1966</th>\n",
       "      <th>d_1967</th>\n",
       "      <th>d_1968</th>\n",
       "      <th>d_1969</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.52</td>\n",
       "      <td>...</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   d_1790  d_1791  d_1792  d_1793  d_1794  d_1795  d_1796  d_1797  d_1798  \\\n",
       "0    0.50    0.41    0.35    0.60    0.66    0.55    0.50    0.56    0.50   \n",
       "1    0.40    0.40    0.31    0.47    0.49    0.38    0.36    0.33    0.31   \n",
       "\n",
       "   d_1799  ...  d_1960  d_1961  d_1962  d_1963  d_1964  d_1965  d_1966  \\\n",
       "0    0.52  ...    1.10    1.49    1.33    0.99    0.98    0.89    1.00   \n",
       "1    0.31  ...    0.14    0.25    0.33    0.29    0.35    0.34    0.32   \n",
       "\n",
       "   d_1967  d_1968  d_1969  \n",
       "0    1.10    1.30    1.27  \n",
       "1    0.33    0.35    0.34  \n",
       "\n",
       "[2 rows x 180 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_ensemle_predictions_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Conclusions</h3>\n",
    "\n",
    "1. We use the validation days(1914-1941) sales for hyper-parameter tuning (since we have the true sales values for these validation days) the different base-learners and meta-model combinations as well as the number of base-learners.\n",
    "\n",
    "2. After trying with different base-learners and meta-model combinations we observe that XGBRegressor as base-learner and LGBMRegressor as meta-model gives lower WRMSSE benchmark.\n",
    "\n",
    "3. Tuning the number of base-learners we observe that for 'N'(number of base-learners) = 18, gives lower WRMSSE.\n",
    "\n",
    "4. Getting validation days(1914-1941) sales and evaluation days(1942-1969) sales and after submitting in the given format we get the leaderboard score of **0.66282** which ranks **352** out of **5558** participants and stands in **top 7%**.\n",
    "\n",
    "5. Of all the models Custom ensemble model performs well as it is able to forecast sales with lower score(WRMSSE) = 0.66282 private score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Kaggle score of all models\n",
    "<img src=\"https://imgur.com/xKld7GP.png\"/>\n",
    "\n",
    "##### Kaggle leaderboard rank of best performing model\n",
    "<img src=\"https://imgur.com/Ia3Hn1s.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
